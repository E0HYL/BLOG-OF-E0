---
layout: post
title: S&P-2019-Asm2Vec
description: "Asm2Vec论文笔记：基于Doc2Vec将汇编函数向量化"
modified: 2020-3-30
tags: [NLP, Papers]
math: true
image:
  feature: abstract-11.jpg
---
<details open><!-- 可选open -->
<summary>Contents</summary>
<div markdown="1">
<!-- TOC -->

- [PV-DM](#pv-dm)
- [Asm2Vec](#asm2vec)
    - [CFG序列化](#cfg%E5%BA%8F%E5%88%97%E5%8C%96)
    - [Asm2Vec模型](#asm2vec%E6%A8%A1%E5%9E%8B)

<!-- /TOC -->
</div>
</details>

**Assembly Code Representation Learning**: 本文表示学习的对象是汇编代码中的一段**函数（function）**，顺带也能获得每个token的。

基于NLP中的`PV-DM`模型，该模型基于tokens学习文档的表示 `Doc2Vec`。

区别：文档是顺序排列的（`sequentially laid out`），汇编代码可被视作有特定语法的图（`a graph and has a specific syntax`）。

## [PV-DM](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)

句向量的分布记忆模型（Distributed Memory Model of Paragraph Vectors），同时学习每个词和整个段落的表示（jointly learn vector representations for each word and each paragraph）

在由一系列段落组成的语料库 $$T$$ 中，给定一个由多个句子 $$s$$ 组成的段落 $$p$$ , 在每个句子 $$s$$ 上使用一个大小为 $$2k+1$$ 的滑动窗口采样，每步向前移动一个单词 $$w$$ 。在每一步中，如下图所示：

- 中间一个词作为目标（target），而两边的词作为上下文（context），完成多分类任务。
- 通过段落和词的ID，将它们分别映射到一个对应的向量上。
- 把这些向量加和平均后（经激活函数sigmod），使用多类别分类器（如softmax）预测目标词。
- 分类错误被反向传播，用于更新词和段落向量。

<figure><img src="{{ site.url }}/images/PV-DM.PNG" style="zoom:65%;" /><figcaption>图1 PV-DM模型</figcaption></figure>

<!--more-->

该模型的目标即最大化对数似然概率：

$$\sum_{p}^{T}\sum_{s}^{p}\sum_{t=k}^{\vert{s}\vert-k}logP(w_t\vert{p},w_{t-k},...,w_{t+k}) \tag{1}$$

## Asm2Vec

> Assembly code carries richer syntax than plaintext. It contains **operations**, **operands**, and **control flow** that are structurally different than plaintext. 
>
> These differences require a **different model architecture** design that cannot be addressed by PV-DM. Next, we present a representation learning model that **integrates the syntax of assembly code**.

### CFG序列化

首先有选择地扩展CFG上的callee，然后通过边采样以及随机游走的方法产生汇编代码序列。

1. 选择性内联被调用函数

   原因：函数内联，即编译器将指定的函数体插入并取代每一处调用该函数的地方（上下文），从而节省了每次调用函数带来的额外时间开支。该优化会很大程度的改变CFG，给汇编克隆搜索带来了很大的挑战。

   本文选择满足以下条件的被调用函数进行內联：

   - 非库函数。因为模型能较好地获取`library call token`的词法语义（`lexical semantic`）。
   - 只內联图上的一阶被调用函数。递归內联会过度扩展caller。
   - 沿用 *BinGo* 的 decoupling metric（式2）和阈值（0.01）。
   - caller大于十行指令时，以式3的 $$\delta<0.6$$ 过滤太长的被调用函数。caller（$$f_s$$）小于十行指令时保留內联，是为了容纳包装函数（`wrapper function`）。

$$\alpha(f_c)=outdegree(f_c)/(outdegree(f_c)+indegree(f_c)) \tag{2}$$

$$\delta(f_s,f_c)=length(f_c)/length(f_s) \tag{3}$$

2. 边采样

   从 callee-expanded CFG 上随机采样边，直到每条边都被覆盖。连接每条边前后的汇编代码形成序列。这种做法可以在CFG中的基本块被拆分或合并的情况下，仍能产生相似的序列。

3. 随机游走

   该方法获得的序列比2中长很多，同时提升了`Dominator`的基本块的优先级。（Dominator是控制流分析中的概念：若基本块B向下执行必须经过基本块A，则称A dominates B）

### Asm2Vec模型

汇编代码函数库 $$RP$$ 中的每个汇编函数 $$f_s$$ 由序列集 $$S(f_s)=seq[1:i]$$ 表示，一个序列 $$seq_i$$ 由一个指令集 $$I(seq_i)=in[1:j]$$ 表示。而一条指令 $$in_j$$ 包含一个操作码 $$P(in_j)$$ 和一个操作数列表 $$A(in_j)$$，将它们连接起来则是标记列表 $$\tau(in_j)=P(in_j)\vert\vert{A(in_j)}$$，其中常量被标准化为它们的十六进制形式。

对于每一个指令序列 $$seq_i$$ ，以大小为3的滑动窗口向前移动，模型的目标即最大化以下对数似然函数：

$$\sum_{f_s}^{RP}\sum_{seq_i}^{S(f_s)}\sum_{in_j}^{seq_i}\sum_{t_c}^{\tau(in_j)}logP(t_c\vert{p,in_{j-1},in_{j+1}}) \tag{4}$$

> 对应关系：句子 - 序列，词 - 指令；最后多出一个加和，是因为指令可进一步拆分为操作码和操作数

<figure><img src="{{ site.url }}/images/asm2vec.PNG"  /><figcaption>图2 Asm2Vec模型</figcaption></figure>

<figure><img src="{{ site.url }}/images/tokenvec.PNG"  /><figcaption>图3 Asm2Vec生成的d维token向量的T-SNE聚类可视化</figcaption></figure>

每个标识（token）被映射为 $$d$$ 维的向量 $$v_t^{\rightarrow}$$ ，以及 $$2\times{d}$$ 维的映射 $$v_{t'}^{\rightarrow}$$。前者用于生成上下文指令的向量，后者用于（目标指令）标识的预测。

每条指令 $$in$$ 表示为操作码 $$d$$ 维向量与 $$d$$ 维操作数向量均值的连接，即 $$2\times{d}$$ 维的 $$C\tau{(in)}$$ ，如式5所示。

$$C\tau{(in)}=v_{P_{(in)}}^{\rightarrow}\vert\vert\frac{1}{\vert{A(in)}\vert}\sum_t^{A(in)}v_{t_b}^{\rightarrow} \tag{5}$$

因而每个函数 $$f_s$$ 也被映射为 $$2\times{d}$$ 维的向量，记作 $$\theta_{f_s}^{\rightarrow}$$ 。

训练时， $$\theta_{f_s}^{\rightarrow}$$ 与目标指令前后两条指令的向量平均后得 $$\delta(in_j,f_s)$$ ，则式4中的概率项可表示为 $$P(t_c\vert\delta(in_j,f_s))$$ 。

对于目标指令 $$\tau(in_j)$$ 中的每一个标识 $$t_c$$，查找 $$2\times{d}$$ 维的映射 $$v_{t'}^{\rightarrow}$$，则上述概率项可建模为softmax多项回归问题：

$$P(t_c\vert\delta(in_j,f_s))=P(v_{t'_c}^{\rightarrow}\vert\delta(in_j,f_s))=\frac{f(v_{t'_c}^{\rightarrow},\delta(in_j,f_s))}{\sum_d^Df(v_{t'_d}^{\rightarrow},\delta(in_j,f_s))} \tag{6}$$

其中， $$f(v_{t'_c}^{\rightarrow},\delta(in_j,f_s))=Uh((v_{t'_c}^{\rightarrow})^T\times\delta(in_j,f_s))$$， $$Uh$$ 为sigmod激活函数。

> 就是这一步f的计算要求token是2*d维的，个人感觉此部分token两种维度embedding的设计有点牵强。

为什么不按式7优化？更加直接：

$$\sum_{f_s}^{RP}\sum_{seq_i}^{S(f_s)}\sum_{in_j}^{seq_i}logP(in_j\vert{p,in_{j-1},in_{j+1}}) \tag{7}$$ 

其中的概率项 $$P(in_j\vert\delta(in_j,f_s))=P(C\tau{(in_j)}\vert\delta(in_j,f_s))$$
